{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Statement:\n",
    "\n",
    "## Marina Pier Inc. is leveraging technology to automate their operations on the San Francisco port.\n",
    "\n",
    "## The companyâ€™s management has set out to build a bias-free/ corruption-free automatic system that reports & avoids faulty situations caused by human error. Examples of human error include misclassifying the correct type of boat. The type of boat that enters the port region is as follows\n",
    "\n",
    "* Buoy\n",
    "* Cruise_ship\n",
    "* Ferry_boat\n",
    "* Freight_boar\n",
    "* Gondola\n",
    "* Inflatable_boat\n",
    "* Kayak\n",
    "* Paper_boat\n",
    "* Sailboat\n",
    "\n",
    "## Marina Pier wants to use Deep Learning techniques to build an automatic reporting system that recognizes the boat. The company is also looking to use a transfer learning approach of any lightweight pre-trained model in order to deploy in mobile devices.\n",
    "\n",
    "## As a deep learning engineer, our task is to:\n",
    "\n",
    "1. Build a CNN network to classify the boat.\n",
    "2. Build a lightweight model with the aim of deploying the solution on a mobile device using transfer learning. We can use any lightweight pre-trained model as the initial (first) layer. MobileNetV2 is a popular lightweight pre-trained model built using Keras API.\n",
    "\n",
    "# Dataset and Data Description: \n",
    "\n",
    "## The dataset folder is called Automating_Port_Operations_dataset \n",
    "\n",
    "## The dataset contains images of 9 types of boats. It contains a total of 1162 images. The training images are provided in the directory of the specific class itself. \n",
    "## Classes:\n",
    "* ferry_boat\n",
    "* gondola\n",
    "* sailboat\n",
    "* cruise_ship\n",
    "* kayak\n",
    "* inflatable_boat\n",
    "* paper_boat\n",
    "* buoy\n",
    "* freight_boat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a CNN to classify the boat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the path for the dataset and displaying first image from each image directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining base path where dataset is located\n",
    "data_dir = os.path.join(os.getcwd(), 'Automating_Port_Operations_dataset')\n",
    "\n",
    "# Getting only visible directories (exclude hidden directories like .DS_Store)\n",
    "image_dirs = [d for d in os.listdir(data_dir) if not d.startswith('.')]\n",
    "\n",
    "print(image_dirs) # List of directories in the dataset\n",
    "print(data_dir) # Path to parent directory of dataset\n",
    "print()\n",
    "\n",
    "# Function to display single image given an image path\n",
    "def display_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "# Displaying the first image in each directory in image_dirs\n",
    "for d in image_dirs:\n",
    "    image_path = os.path.join(data_dir, d, os.listdir(os.path.join(data_dir, d))[0])\n",
    "    print(f\"Displaying first image from folder: {d}\")\n",
    "    display_image(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training and datasets with following characteristics:\n",
    "\n",
    "* Dataset will be split into train and test in the ratio 80:20, with shuffle and random state = 43\n",
    "* We'll use tf.keras.preprocessing.image_dataset_from_directory to load the train and test datasets with data normalization\n",
    "\n",
    "* We'll load train, validation and test dataset in batches of 32 using the function initialized in step above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Split into Training, Validation and Testing with normalzation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing images from training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Convolutional Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll build a CNN network with Keras with the following layers:\n",
    "\n",
    "### We'll do data augmentation issue to balance the data as we have some images that are \n",
    "\n",
    "### Conv2D with 32 filters, kernel size 3,3, and activation relu, followed by MaxPool2D\n",
    "### Conv2D with 32 filters, kernel size 3,3, and activation relu, followed by MaxPool2D\n",
    "### GLobalAveragePooling2D layer\n",
    "### Dense layer with 128 neurons and activation relu\n",
    "### Dense layer with 128 neurons and activation relu\n",
    "### Dense layer with 9 neurons and activation softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with Adam optimizer, categorical_crossentropy loss, and with metrics accuracy, precision, and recall.\n",
    "Train the model for 20 epochs and plot training loss and accuracy against epochs.\n",
    "Evaluate the model on test images and print the test loss and accuracy.\n",
    "Plot heatmap of the confusion matrix and print classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define data directory\n",
    "data_dir = os.path.join(os.getcwd(), 'Automating_Port_Operations_dataset')\n",
    "\n",
    "# Create training dataset\n",
    "train_val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,  # 80% for training, 20% for validation + test\n",
    "    subset=\"training\",\n",
    "    seed=43,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Create validation and test datasets\n",
    "val_test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2, # 20% for validation + test \n",
    "    subset=\"validation\",\n",
    "    seed=43,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_val_ds.class_names  # Get class names    \n",
    "print(f\"Class names: {class_names}\")\n",
    "\n",
    "# Further split val_test_ds into validation and test sets\n",
    "val_batches = int(0.5 * len(val_test_ds)) # Split the validation and test sets in half\n",
    "\n",
    "# Isolate the validation and test datasets\n",
    "val_ds = val_test_ds.take(val_batches)\n",
    "test_ds = val_test_ds.skip(val_batches)\n",
    "\n",
    "# Normalize the pixel values\n",
    "def normalize_img(image, label):\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "def one_hot_encode(image, label):\n",
    "    label = tf.one_hot(label, depth=len(class_names))  # One-hot encode the labels (convert to binary vectors) in order to use categorical crossentropy loss\n",
    "    return image, label\n",
    "\n",
    "# Apply this to the datasets\n",
    "train_ds = train_val_ds.map(lambda x, y: (x / 255.0, tf.one_hot(y, len(class_names)))).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) # Cache, shuffle, and prefetch the training dataset\n",
    "val_ds = val_ds.map(lambda x, y: (x / 255.0, tf.one_hot(y, len(class_names)))).cache().prefetch(buffer_size=AUTOTUNE) # Cache and prefetch the validation dataset\n",
    "test_ds = test_ds.map(lambda x, y: (x / 255.0, tf.one_hot(y, len(class_names)))).cache().prefetch(buffer_size=AUTOTUNE) # Cache and prefetch the test dataset\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_shape = (img_height, img_width, 3) # Define the input shape because we are using a Sequential model\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),  # Explicit Input layer to define the input shape\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'), # Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
    "    layers.MaxPooling2D(), # Max pooling layer\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'), # Convolutional layer with 32 filters, 3x3 kernel size, and ReLU activation\n",
    "    layers.MaxPooling2D(), # Max pooling layer\n",
    "    layers.GlobalAveragePooling2D(), # Global average pooling layer which averages the values in the feature maps\n",
    "    layers.Flatten(), # Flatten layer to flatten the output of the previous layer\n",
    "    layers.Dense(128, activation='relu'), # Dense layer with 128 units and ReLU activation\n",
    "    layers.Dense(128, activation='relu'), # Dense layer with 128 units and ReLU activation\n",
    "    layers.Dense(len(class_names), activation='softmax') # Output layer with units equal to the number of classes and softmax activation\n",
    "])\n",
    "\n",
    "model.summary() # Display the model summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam', # Adam optimizer\n",
    "    loss='categorical_crossentropy', # Categorical crossentropy loss\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()] # Accuracy, precision, and recall metrics\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# Set the number of epochs to 20\n",
    "epochs = 20\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds, # Use the training dataset for training\n",
    "    validation_data=val_ds,  # Use the validation dataset for validation during training\n",
    "    epochs=epochs # Set the number of epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training and Validation Accuracy/Loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy/loss over epochs\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs) # Define the range of epochs\n",
    "\n",
    "plt.figure(figsize=(12, 6)) # Set the figure size\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_ds)\n",
    "\n",
    "# Print test loss, accuracy, precision, and recall\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Test Accuracy: {test_accuracy}')\n",
    "print(f'Test Precision: {test_precision}')\n",
    "print(f'Test Recall: {test_recall}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model on test images and print test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Generate predictions from the model on the test set\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the test dataset to get true labels and predicted labels\n",
    "for images, labels in test_ds:\n",
    "    predictions = model.predict(images)\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))  # Convert predicted probabilities to class labels\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot encoded labels back to integers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ploat heatmap of confusion matrix and print classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build a lightweight model with the aim of deploying the solution on a mobile device using transfer learning, by using MobileNetV2, a lightweight pre-trained model using Keras API as the initial (first) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32\n",
    "img_height = 224  # Updated to match MobileNetV2 expected input size\n",
    "img_width = 224\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define data directory\n",
    "data_dir = os.path.join(os.getcwd(), 'Automating_Port_Operations_dataset')\n",
    "\n",
    "# Function to normalize the pixel values and one-hot encode labels\n",
    "def one_hot_encode(image, label):\n",
    "    label = tf.one_hot(label, depth=9)  # Assuming 9 classes in the dataset\n",
    "    image = image / 255.0  # Normalize the image to the range [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Load the dataset and split it into train and validation (70% train, 30% validation)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    seed=1,\n",
    "    image_size=(img_height, img_width),  # Set to 224x224 to match MobileNetV2\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=1,\n",
    "    image_size=(img_height, img_width),  # Set to 224x224 to match MobileNetV2\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Apply normalization and one-hot encoding to the datasets\n",
    "train_ds = train_ds.map(one_hot_encode).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.map(one_hot_encode).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Load the MobileNetV2 pre-trained model with weights from ImageNet\n",
    "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                         include_top=False,  # We don't need the top layers (classification layers)\n",
    "                         weights='imagenet')  # Load pre-trained ImageNet weights\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    base_model,  # Add MobileNetV2 as the base layer\n",
    "    layers.GlobalAveragePooling2D(),  # Reduce dimensionality\n",
    "    layers.Dropout(0.2),  # Dropout for regularization\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(9, activation='softmax')  # Assuming 9 output classes (one-hot encoded)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',  # Use categorical crossentropy for one-hot encoded labels\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "#model.summary()\n",
    "\n",
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(val_ds)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training vs Validation Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Dataset (Train/Test 70:30) using tf.keras.preporcessing.image_dataset_from_directory to load and split the dataset, shuffle the data, and set a random seed to ensure consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 32\n",
    "img_height = 224  # Updated to match MobileNetV2 expected input size\n",
    "img_width = 224\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Define data directory\n",
    "data_dir = os.path.join(os.getcwd(), 'Automating_Port_Operations_dataset')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize dataset pixel values and one-hot encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the pixel values and one-hot encode labels\n",
    "def one_hot_encode(image, label):\n",
    "    label = tf.one_hot(label, depth=9)  # Assuming 9 classes in the dataset\n",
    "    image = image / 255.0  # Normalize the image to the range [0, 1]\n",
    "    return image, label\n",
    "\n",
    "# Load the dataset and split it into train and validation (70% train, 30% validation)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"training\",\n",
    "    seed=1,\n",
    "    image_size=(img_height, img_width),  # Set to 224x224 to match MobileNetV2\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.3,\n",
    "    subset=\"validation\",\n",
    "    seed=1,\n",
    "    image_size=(img_height, img_width),  # Set to 224x224 to match MobileNetV2\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Apply normalization and one-hot encoding to the datasets\n",
    "train_ds = train_ds.map(one_hot_encode).cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.map(one_hot_encode).cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN with Transfer Learning Using MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MobileNetV2 pre-trained model with weights from ImageNet\n",
    "base_model = MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                         include_top=False,  # We don't need the top layers (classification layers)\n",
    "                         weights='imagenet')  # Load pre-trained ImageNet weights\n",
    "\n",
    "# Freeze the base model\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the CNN model\n",
    "model = models.Sequential([\n",
    "    base_model,  # Add MobileNetV2 as the base layer\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.2),  # Dropout for regularization\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.1),\n",
    "    layers.Dense(9, activation='softmax')  # Assuming 9 output classes\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',  # Use categorical crossentropy for one-hot encoded labels\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "# Display the model summary\n",
    "####model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(val_ds)\n",
    "\n",
    "# Print evaluation results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation accuracy and loss\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training vs Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix and plot it as a heatmap\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Generate predictions from the model on the validation set\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# Iterate over the validation dataset to get true labels and predicted labels\n",
    "for images, labels in val_ds:\n",
    "    predictions = model.predict(images)\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))  # Convert predicted probabilities to class labels\n",
    "    y_true.extend(np.argmax(labels.numpy(), axis=1))  # Convert one-hot encoded labels back to integers \n",
    "    \n",
    "# Compute the confusion matrix  \n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print the classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
